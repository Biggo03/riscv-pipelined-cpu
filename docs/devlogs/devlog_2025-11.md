# November

## November 1st - 6th

### Notes
Over these days, I completed the implementation of csr functionality. This included routing the csr_data directly through the pipeline so that it could be used for forwarding,
and writing back to the standard register file, forwarding the neccesary signals back for the csr_alu, and fixing any bugs introduced or found during this implementation.

Once everything was routed as needed, and the current system smoke test passed, I created the csr testing program. This program was a number of sections, each with their own set of csr instructions.
each of these sections finished with a branch to a label that would write a failure value to the success flag memory location. In this way, the assembly program only passes if the
operation in each section pass.

To do this, it was easiest to not use the implemented cycle counters to test the instructions, as their values would be harder to pin down and verify. Becase of this, the mstatus register
was added to essentially act as a test register in this case.

During testing, a very important bug was also found in the instruction cache controller. It was found that the delay guard FSM was exiting the DELAYING state improperly.
The cause of this was that the state change condition was: (**~**instr_hit_f_o | pc_src_reg_i[0]) rather than (instr_hit_f_o | pc_src_reg_i[0]). This caused the fsm to improperly
oscillate states when a cache miss and branch hit/miss occured in the same cycle.

Essentially, the condition to switch only when there is a hit, is to ensure that all data is in place before allowing the pipeline to contiue operating. When the
condition is flipped, the cache effectively exits replacement while the replacement data continues to fill in. This leave the block only partially filled, meaning that
a "cache hit" can not occur, stalling the system indefinitely.

This issue was likely caused when I changed the instr_miss signl to instr_hit, missing that inversion.

This reveals that my testing for cache integration and coherency were not extensive, and means that I will likely revisit this to ensure that all edge cases are properly handelled.

In additon to this, I made small QOL improvements to my test driver. These include the option to list all available tests, as well as the use of the ordered set type to ensure that
regressions always run tests in the order listed within the test catalog.

### Summary
- Completed implementation functionality needed for csr instructions
- Tested CSR instrucion operation
- Added mstatus csr
- Fixed instruction cache controller bug
- QOL improvements for test driver

## November 8th

### Notes:
Today I focused on updating the documentation to reflect the current state of design, and reworking my standard test pass condition. I won't delve into the documentation, but I will give an overview of the standard test pass condition

Previously. the test pass condition was writing the decimal value 25 to the decimal memory location 100. This was a little clunky, and I could see it causing issues later on for more tests, as it didn't really make sense considering my memory map. So I decided to make use of my new csr functionality, and created a custom csr, mtest_status. This register will essentially act as the pass fail flag. When ABCD1234 is written it is seen as a pass, when DEADBEEF is written, it is seen as a fail.

I believe this is more robust, configurable, extensible and easier to monitor. Rather than writing an arbitrary value to an arbitrary memory location, the location (register), and value now have a distinct meaning. I can add new values with new meanings easily across both the hardware, and software portions of the design, or add new CSR's to hold new information.

While doing this, I added assembly macros, and C inline functions to write the pass and fail values to the register of interest to make inserting the pass/fail condition in software tests easy. In addition, I wrote basic inline functions to write values to any generic CSR register.

I also began automatically generating a C header file with all CSR addresses, which will allow for easy CSR access whenever software is being written for the processor.

### Summary:
- Consolidated documentation
- Updated test pass condition
- Created CSR read and write functions for C code
- Created test pass macros for assembly
- Automatically generate C header file containing csr addresses

## November 9th:

### Notes:
Today I prepped my tb directory, and test driving script for the upcoming improvements to the testing architecture. This included seperating the system-level test from the module level tests. This is because the system level test will have more components made specifically for it (for example the main memory model). So now my tb/system_test contains the top testbench, and the components it instantiates.

In addition to this, I also began looking into synthesis again, as I thought it was a good time to ensure that the addition of the csr's didn't have a large affect on fmax, or area. Unforunately, when I checked, there was a massive blowup in area, about ~18,000 LUT's (subtracting datamem luts). Upon further investigation, I believe that this blow up is far more likely due to branching buffer being synthesized properly, rather than the csr addition.

I believe the LUT blowup is due to the fix to the local predictors made a while back, in which an inferred latch was removed. I believe that this inferred latch allowed for optimization that is no longer possible. I'm not sure how it was optimized, but nonetheless, it isn't anymore. This hypothesis is strengthened by the fact that when re-synthesizing the branching buffer, it now shows anywhere from 11k-15k LUTs, depending on settings and tweaks to RTL setup.

This initially seemed to be due to a huge number of control sets generated by the local predictors. A control set is a unique combo of clk, rst, and en signals. Each local predictor had it's own enable, and every 4 local predictors had it's own rst. Control sets reduce area effeciency because one CLB, containing 8 registers, and 4 6-input LUTs can only deal use one control set.

The initial solution was to use synthesis directives to extract the en and rst signals from the registers to reduce the number of control sets. This was successful in reducing the number of control sets from ~4150 down to ~20, but was not successful in reducing area. This is almost certainly due to the extra area associated with pushing enable and reset handelling to the D pin logic.

So, this leaves the question of how to move forward? Leaving > 4000 control sets is unfeasible, and pushing the rst and en logic to the D-pin still explodes area. With this in mind, I think that there is only one real option to investigate if branch prediction is to remain a feature of the architecture. And that option is to move from LUTRAM, and registers, to block ram.

the zynq logic family supports 18 and 36 K-bit block rams. These primitives could hold both target addresses, and local predictor outputs. This has several huge advantages:
1. Extremely low LUT and register usage
    - BRAMs will handle address decoding, and storage, removing the need for LUTs in decoding (previous cost was likely close to ~4000 LUTs), and storage (previous cost was ~700 LUTs).
    - This also removes the need for registers, reducing register cost by at least 8096 registers (2-bits per local predictor, 4096 local predictors)
2. Reduces critical path
    - It's very likely that BRAM primitives are faster at decoding their large address space than whatever could be synthesized using LUTs. Decoding 4096 local predictor outputs likely accounted for a sizeable portion of the critical path.
        - Target address decoding was part of the fetch stage, so is less likely to be part of critical path.

That said, there is also one big tradeoff here: Instruction latency, and branch prediction penalty. The block rams are synchronous read only, therefore there is a cycle delay in between the time they recieve their index, and the time the information will be ready. This essentially means that there will be an extra cycle penalty for all branch misses, and an extra cycle latency between instruction kickoff and completion.

Another smaller thing that is of note, is that the data needed doesn't fit the bram perfectly. With 1024, 32-bit target addresses, and 4096 2-bit predictor outputs, this gives 40960 bits, more than the 36864 bits the largest bram can hold. This give two options:
1. Use one 36 K-bit bram for target entries and one 18 K-bit bram for the predictor outputs.
2. Cut reduce the number of local predictors per index from 4 down to 2
    - Allows for the usage of one 36 K-bit bram

Honestly, option one seems much more attractive, and the extra space gives incentive to look for more ways to improve prediction accuracy (given the extra space, and LUT savings)

With all of that said, this is something that I will likely look into more once my testing infrastrucrure is improved, as that is intended to be my main focus as of now. It was good that this issue was discovered, however there is no quick solution, I will need to make an architectural change. So this fix will likely be done when looking into the 6th pipeline stage, perhaps a 7th stage will also be added depending on the needs of the design. I will update my roadmap according to this.

### Summary:
- Updatedtb directory structure
- Updated test driver to fit new tb directory structure
- Looked into branch prediction synthesis issues and possible solutions

## November 12th

### Notes:
Today I'm planning on looking into the planned improvements for my testing infrastructure. I've decided to continue using monitor modules, as this will be easier than implementing a bunch of performance csr's, and I can move the functionality to csr's if needed. This will be easier, because I won't need to update anything related to the csrs, it won't need to be as heavily validated, and I can design it with logging in mind.

To start, I think I'm going to have a number of monitors:
- cycle monitor -> already have, will monitor total cycles, retired instructions, and retired instruction type
- branch monitor -> will monitor branch misses, and related penalties
- cache monitor -> Will monitor cache misses and hits, and related penalties

Today I will be working on the branch monitor. The number of branches is already handelled by the cycle monitor, so all that needs to be monitored here, is the number of mispredictions, and their associated penalty. To make sure I had a good idea of how a misprediction penalty can be detected, I decided to look into the **branch_cache_stress_test** directed assembly test wave form.

Recently when I was implementing the CSR functionality into the datapath, I found a cache issue. I fixed the underlying cause, and moved on, but there was a problem there: **branch_cache_stress_test** didn't catch the bug. This clearly showed that the test is out of date.

I realized this again when looking through the waveform to get a better idea of the branching relationships. Long story short, I decided that I should update this test to test the intended functionality, so that's what I mainly ended up doing today.

This fix mostlly involved updating the alignments and number of NOPs to ensure that the branch conditions I wanted to check were occuring. While doing this, I also found an issue with the main memory model. Essentially, it began replacement only on a cache-miss, regardless of if replacement was permitted. This lead to replacement data starting to be fed to the cache one cycle early, messing up alignement, and not providing all intended data. The fix was to update the main mem model to wait on a cache miss, and replacement permission to begin sending data to the cache.

All this said, The **branch_cache_stress_test** has been updated, and the main memory model has also been updated. I think I should update the branch_cache_stress_test to have some more cases, and the issue with main mem demonstrates the importance of a formal communication protocol between memory componeents that will be taken into account when expanding the memory system.

### Summary:
- Layed out plan for monitors
- Updated branch_cache_stress_test
- Fixed issue with main memory model

## November 14th - 15th

### Notes:
During these days, I started out by again wanting to work on my documentation for my instruction cache. But while doing this I realized how confusing my instruction cache sets were designed. It was not very clear how everything worked in a hardware context, as the module was initially designed more like software, with no states, and not a lot of clarity in how things were happenning.

Because of this, I decided it would be best to refactor the sets to be easier to read and understand before documenting them. So I initially refactored it into a state machine with two states, monitor and replace. This small FSM intracted with a flops/RAM that handelled the actual replacement and updates. The overall structure is now designed more like traditional hardware, making it much easier to follow.

A nice bonus of this, is that in synthesis, the area was reduced from a baseline of 168 LUT's per set, down to 157 LUT's per set (about a **6.5%** resource usage improvement)! That's not a huge improvement, but it's definitely helpful. For the whole instruction cache, this translates to a saving of 352 LUT's!

### Summary
- Refactored i-cache sets
- Reduced i-cache resource usage by ~6.5%

## Novemberr 16th

### Notes:
After looking through the i-cache set, the issues I had previously with the branch predictor, the need to restructure the predictor, and my plans to add multiplication instructions, I think it is time to pivot a little bit. Previously, my priorities were to be **testing/benchmarking infrastructure**, and **block documentation**. I think that considering everything that has come to light since I've continued working on this project, and what I want for it, my new priorities should be **testing/benchmarking infrastructure**, and a **pipeline redesign**.

These are for a number of factors. The testing/benchmarking infrastructure has the same motivation as before, I want quantitative measures for the effectiveness of the architecture, and I want infrastructure to improve debugging access. These will be useful no matter what I do next, and I think I'm building a better idea of what that should look like.

The reason I want to do a pipeline restructure is because, I believe there are additions (mult and data-cache), and fixes (branch prediction rework) that require changes to the pipeline. So I think it makes sense to make a high level plan for these changes so that I have a guideline for how to develop all of these features. Essentially the idea is to provide a more stable working framework for me to develop in, rather than having moving goal posts. After exploring the design more, and getting a better understanding of the issues it faces, timing, and area wise, I feel I am in a decent position to make these choices.

The testing infrastructure will ease future debugging, and give me confidence in the changes being made, and the architecture framework will give me guidance in how to develop.

Again, I think first step is to improve testing, before exploring the architecture improvements. As of writing this, in my roadmap I list a ton of things I want to monitor, and I think that those are good to have, but I don't currently have any plans for logging and comparing these. In addition to that, I think those are great metrics to have, but for debugging, it won't be as useful.

I think that honestly, the best thing I can do is implement logging for pipeline stages in a clear formatted way, so I think that that's what my main focus will be on today.



